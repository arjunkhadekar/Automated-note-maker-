# -*- coding: utf-8 -*-
"""tva.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/100MKRMGDh86D1Qm5Kj-gIi1XirqxJ72Q
"""

# Install required packages (run only once)
!pip install pymupdf youtube-transcript-api transformers nltk reportlab

!pip install SpeechRecognition

import fitz  # PyMuPDF
import speech_recognition as sr
from youtube_transcript_api import YouTubeTranscriptApi
from transformers import pipeline
import nltk
import re
from google.colab import files
import os
from reportlab.lib.pagesizes import letter
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, ListItem, ListFlowable
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib import colors
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Download NLTK data (run only once)
nltk.download('punkt', quiet=True)
nltk.download('stopwords', quiet=True)

print("âœ… Libraries imported successfully!")

# Initialize the summarization pipeline
summarizer = pipeline("summarization", model="facebook/bart-large-cnn", tokenizer="facebook/bart-large-cnn")

def extract_title(text, source_name=""):
    """Extract a title from the text or generate one based on content"""
    # Try to find a title in the first few sentences
    first_sentences = nltk.sent_tokenize(text[:500])
    if first_sentences and len(first_sentences[0]) < 100:
        return first_sentences[0].strip()

    # If no good title found, generate one from the content
    if source_name:
        return f"Notes on {source_name}"
    else:
        # Use the first 5-7 words as a title
        words = text.split()[:7]
        return " ".join(words) + "..."

def create_abstractive_summary(text, max_length=200, min_length=50):
    """Create an abstractive summary of the text using BART model"""
    if len(text.strip()) == 0:
        return "No text found to summarize."

    # Clean the text
    text = text.replace('\n', ' ')

    # For long texts, chunk and summarize each part
    chunks = []
    max_chunk_length = 1024  # BART can handle longer inputs

    # Split text into chunks
    if len(text) > max_chunk_length:
        words = text.split()
        current_chunk = []
        current_length = 0

        for word in words:
            if current_length + len(word) + 1 <= max_chunk_length:
                current_chunk.append(word)
                current_length += len(word) + 1
            else:
                chunks.append(' '.join(current_chunk))
                current_chunk = [word]
                current_length = len(word) + 1

        if current_chunk:
            chunks.append(' '.join(current_chunk))
    else:
        chunks = [text]

    # Summarize each chunk
    summaries = []
    for chunk in chunks:
        summary = summarizer(chunk, max_length=max_length, min_length=min_length,
                            do_sample=False, num_beams=4)[0]['summary_text']
        summaries.append(summary)

    return "\n".join(summaries)

def extract_key_points(text, num_points=5):
    """Extract key points from the text"""
    sentences = nltk.sent_tokenize(text)

    # If we have very few sentences, use all of them
    if len(sentences) <= num_points:
        return sentences

    # Otherwise, select evenly distributed sentences
    step = len(sentences) // num_points
    key_points = [sentences[i] for i in range(0, len(sentences), step)][:num_points]

    return key_points

def generate_conclusion(summary):
    """Generate a conclusion based on the summary"""
    # Use the last sentence of the summary as a starting point
    sentences = nltk.sent_tokenize(summary)
    if sentences:
        conclusion = f"In conclusion, {sentences[-1].lower()}"
        return conclusion
    return "In conclusion, this material provides valuable insights on the topic."

print("âœ… Helper functions defined!")

def create_structured_notes(text, source_name=""):
    """Create structured notes with title, content, and conclusion"""
    # Extract or generate a title
    title = extract_title(text, source_name)

    # Create the main content summary
    summary = create_abstractive_summary(text)

    # Extract key points from the summary
    key_points = extract_key_points(summary)

    # Generate a conclusion
    conclusion = generate_conclusion(summary)

    # Structure the notes
    notes = {
        "title": title,
        "date": datetime.now().strftime("%B %d, %Y"),
        "summary": summary,
        "key_points": key_points,
        "conclusion": conclusion
    }

    return notes

def generate_pdf(notes, filename="class_notes.pdf"):
    """Generate a PDF from the structured notes"""
    doc = SimpleDocTemplate(filename, pagesize=letter)
    styles = getSampleStyleSheet()

    # Create custom styles
    title_style = ParagraphStyle(
        'TitleStyle',
        parent=styles['Title'],
        fontSize=16,
        textColor=colors.darkblue,
        spaceAfter=12
    )

    heading_style = ParagraphStyle(
        'HeadingStyle',
        parent=styles['Heading2'],
        fontSize=14,
        textColor=colors.darkblue,
        spaceBefore=12,
        spaceAfter=6
    )

    normal_style = styles['Normal']

    # Build the document content
    content = []

    # Title and date
    content.append(Paragraph(notes["title"], title_style))
    content.append(Paragraph(f"Date: {notes['date']}", styles['Italic']))
    content.append(Spacer(1, 12))

    # Summary section
    content.append(Paragraph("Summary", heading_style))
    content.append(Paragraph(notes["summary"], normal_style))
    content.append(Spacer(1, 12))

    # Key Points section
    content.append(Paragraph("Key Points", heading_style))

    # Create a list of key points
    bullet_points = []
    for point in notes["key_points"]:
        bullet_points.append(ListItem(Paragraph(point, normal_style)))

    content.append(ListFlowable(
        bullet_points,
        bulletType='bullet',
        start='â€¢'
    ))

    content.append(Spacer(1, 12))

    # Conclusion section
    content.append(Paragraph("Conclusion", heading_style))
    content.append(Paragraph(notes["conclusion"], normal_style))

    # Build the PDF
    doc.build(content)

    return filename

print("âœ… Note generation and PDF functions defined!")

def extract_text_from_pdf(file_path):
    """Extract text from a PDF file - FIXED VERSION"""
    text = ""
    doc = fitz.open(file_path)
    # Iterate through each page and extract text
    for page_num in range(len(doc)):
        page = doc[page_num]
        text += page.get_text()
    doc.close()
    return text

def transcribe_audio(file_path):
    """Transcribe audio to text"""
    recognizer = sr.Recognizer()
    with sr.AudioFile(file_path) as source:
        audio = recognizer.record(source)
    try:
        text = recognizer.recognize_google(audio)
        return text
    except sr.UnknownValueError:
        return "Error: Could not understand the audio."
    except sr.RequestError:
        return "Error: Could not request results. Check your internet connection."

def extract_video_id(url):
    """Extract YouTube video ID from URL"""
    match = re.search(r"(?:v=|\/)([0-9A-Za-z_-]{11}).*", url)
    return match.group(1) if match else None

def fetch_youtube_transcript(url):
    """Fetch and process YouTube transcript"""
    video_id = extract_video_id(url)
    if not video_id:
        return "Invalid YouTube URL", None
    try:
        transcript = YouTubeTranscriptApi.get_transcript(video_id)
        full_text = " ".join([t['text'] for t in transcript])
        title = f"YouTube Video {video_id}"
        return full_text, title
    except Exception as e:
        return f"Error: {str(e)}", None

print("âœ… Content processing functions defined!")

class NotesGenerator:
    def __init__(self):
        self.current_notes = None
        self.current_pdf = None

    def display_notes(self, notes):
        """Display the notes structure"""
        print("\nðŸ“ NOTES STRUCTURE ðŸ“")
        print(f"Title: {notes['title']}")
        print(f"Date: {notes['date']}")
        print("\nðŸ“Œ Summary:")
        print(notes['summary'])
        print("\nðŸ“Œ Key Points:")
        for i, point in enumerate(notes['key_points'], 1):
            print(f"{i}. {point}")
        print("\nðŸ“Œ Conclusion:")
        print(notes['conclusion'])

    def process_content(self, content_text, source_name):
        """Process content to create structured notes and PDF"""
        print("\nðŸ” Generating notes...")

        # Create structured notes
        notes = create_structured_notes(content_text, source_name)
        self.current_notes = notes

        # Display the notes structure
        self.display_notes(notes)

        # Generate PDF
        pdf_filename = f"{source_name.replace(' ', '_').lower()}_notes.pdf"
        print("\nðŸ“„ Creating PDF...")
        pdf_path = generate_pdf(notes, pdf_filename)
        self.current_pdf = pdf_path

        print(f"\nâœ… PDF generated: {pdf_path}")

        # Make the PDF downloadable in Colab
        print("\nâ¬‡ï¸ Downloading PDF...")
        files.download(pdf_path)

        return notes

    def process_pdf(self):
        """Handle PDF processing"""
        print("\nðŸ“„ PDF to Class Notes Converter")
        uploaded = files.upload()

        if not uploaded:
            print("âŒ No file uploaded.")
            return

        filename = next(iter(uploaded))
        print("\nâ³ Processing PDF...")

        # Fixed PDF text extraction
        text = extract_text_from_pdf(filename)

        print("\nðŸ” Generating notes...")
        source_name = os.path.splitext(filename)[0]
        self.process_content(text, source_name)

    def process_audio(self):
        """Handle audio processing"""
        print("\nðŸŽµ Audio to Class Notes Converter")
        uploaded = files.upload()

        if not uploaded:
            print("âŒ No file uploaded.")
            return

        filename = next(iter(uploaded))
        print("\nâ³ Processing audio...")
        text = transcribe_audio(filename)

        if not text.startswith("Error"):
            source_name = os.path.splitext(filename)[0]
            self.process_content(text, source_name)
        else:
            print(f"âŒ {text}")

    def process_youtube(self):
        """Handle YouTube video processing"""
        print("\nðŸŽ¬ YouTube to Class Notes Converter")
        url = input("Enter YouTube URL: ")

        print("\nâ³ Fetching transcript...")
        transcript, title = fetch_youtube_transcript(url)

        if not transcript.startswith("Error"):
            source_name = title if title else "YouTube Video"
            self.process_content(transcript, source_name)
        else:
            print(f"âŒ {transcript}")

    def main_menu(self):
        """Main menu for the notes generator"""
        print("\nðŸŽ“ CLASS NOTES GENERATOR ðŸŽ“")
        print("This tool creates structured class notes with downloadable PDFs")

        while True:
            print("\nChoose Source Type:")
            print("1ï¸âƒ£ PDF Document")
            print("2ï¸âƒ£ Audio Recording")
            print("3ï¸âƒ£ YouTube Video")
            print("4ï¸âƒ£ Exit")

            mode = input("Enter your choice (1-4): ")

            if mode == "1":
                self.process_pdf()
            elif mode == "2":
                self.process_audio()
            elif mode == "3":
                self.process_youtube()
            elif mode == "4":
                print("ðŸ‘‹ Exiting program. Goodbye!")
                break
            else:
                print("âŒ Invalid choice. Please try again.")

print("âœ… Notes Generator class defined!")

pip install nltk

import nltk
nltk.download('punkt_tab')

import nltk
nltk.find('tokenizers/punkt_tab', paths=nltk.data.path)

# Create and run the notes generator
notes_app = NotesGenerator()
notes_app.main_menu()

pip install vosk

pip install fpdf

model_path = "model/vosk-model-small-en-us-0.15"

!wget https://alphacephei.com/vosk/models/vosk-model-small-en-us-0.15.zip
!unzip vosk-model-small-en-us-0.15.zip -d model

import os
model_path = os.path.abspath("model/vosk-model-small-en-us-0.15")

# Import necessary libraries
from google.colab import files
from pydub import AudioSegment
from vosk import Model, KaldiRecognizer
import json
from transformers import pipeline
from fpdf import FPDF
import os

# Function to upload MP3 file
def upload_mp3_file():
    uploaded = files.upload()
    mp3_file = list(uploaded.keys())[0]  # Get the uploaded file name
    print(f"Uploaded file: {mp3_file}")
    return mp3_file
# Import necessary libraries
from google.colab import files
from pydub import AudioSegment
from vosk import Model, KaldiRecognizer
import json
from transformers import pipeline
from fpdf import FPDF
import os

# Function to upload MP3 file
def upload_mp3_file():
    uploaded = files.upload()
    mp3_file = list(uploaded.keys())[0]  # Get the uploaded file name
    print(f"Uploaded file: {mp3_file}")
    return mp3_file
# Import necessary libraries
from google.colab import files
from pydub import AudioSegment
from vosk import Model, KaldiRecognizer
import json
from transformers import pipeline
from fpdf import FPDF
import os

# Function to upload MP3 file
def upload_mp3_file():
    uploaded = files.upload()
    mp3_file = list(uploaded.keys())[0]  # Get the uploaded file name
    print(f"Uploaded file: {mp3_file}")
    return mp3_file
# Import necessary libraries
from google.colab import files
from pydub import AudioSegment
from vosk import Model, KaldiRecognizer
import json
from transformers import pipeline
from fpdf import FPDF
import os

# Function to upload MP3 file
def upload_mp3_file():
    uploaded = files.upload()
    mp3_file = list(uploaded.keys())[0]  # Get the uploaded file name
    print(f"Uploaded file: {mp3_file}")
    return mp3_file

# Function to convert MP3 to WAV
def convert_mp3_to_wav(mp3_file):
    sound = AudioSegment.from_mp3(mp3_file)
    wav_file = mp3_file.replace(".mp3", ".wav")
    sound.export(wav_file, format="wav")
    return wav_file

mp3_file = upload_mp3_file()

# Import necessary libraries
from google.colab import files
from pydub import AudioSegment

# Function to upload MP3 file
def upload_mp3_file():
    uploaded = files.upload()
    mp3_file = list(uploaded.keys())[0]  # Get the uploaded file name
    print(f"Uploaded file: {mp3_file}")
    return mp3_file

# Function to convert MP3 to WAV
def convert_mp3_to_wav(mp3_file):
    wav_file = "temp.wav"
    audio = AudioSegment.from_mp3(mp3_file)
    audio.export(wav_file, format="wav")
    print("Converted MP3 to WAV.")
    return wav_file

# Main workflow
def main():
    mp3_file = upload_mp3_file()  # Ensure this is called first and assigns a value to mp3_file

    if not mp3_file:
        print("No file uploaded. Exiting...")
        return

    wav_file = convert_mp3_to_wav(mp3_file)  # Pass mp3_file here

    print(f"WAV file created: {wav_file}")

# Run the project in Google Colab environment
if __name__ == "__main__":
    main()

# Import necessary libraries
from google.colab import files
from pydub import AudioSegment
from vosk import Model, KaldiRecognizer
import json
from transformers import pipeline
from fpdf import FPDF
import os

# Function to upload MP3 file
def upload_mp3_file():
    uploaded = files.upload()
    mp3_file = list(uploaded.keys())[0]  # Get the uploaded file name
    print(f"Uploaded file: {mp3_file}")
    return mp3_file

# Function to convert MP3 to WAV
def convert_mp3_to_wav(mp3_file):
    wav_file = "temp.wav"
    audio = AudioSegment.from_mp3(mp3_file)
    audio = audio.set_frame_rate(16000).set_channels(1)  # Ensure compatibility with Vosk
    audio.export(wav_file, format="wav")
    print("Converted MP3 to WAV.")
    return wav_file

# Function for speech-to-text conversion
def speech_to_text(wav_file):
    model_path = os.path.abspath("model/vosk-model-small-en-us-0.15")  # Use absolute path

    # Check if model exists
    if not os.path.exists(model_path):
        print("Model directory does not exist. Downloading model...")
        !wget https://alphacephei.com/vosk/models/vosk-model-small-en-us-0.15.zip
        !unzip vosk-model-small-en-us-0.15.zip -d model
        print("Model downloaded and extracted.")

    try:
        # Initialize Vosk Model
        model = Model(model_path)
        recognizer = KaldiRecognizer(model, 16000)

        # Read audio data and process it
        with open(wav_file, "rb") as f:
            audio_data = f.read()
            recognizer.AcceptWaveform(audio_data)
            result = recognizer.Result()
            transcript = json.loads(result)["text"]
        return transcript
    except Exception as e:
        print(f"Error loading model or processing audio: {e}")
        return ""

# Function to summarize text using Hugging Face Transformers
def summarize_text(text):
    summarizer = pipeline("summarization")
    summary = summarizer(text, max_length=150, min_length=50, do_sample=False)
    return summary[0]["summary_text"]

# Function to generate structured notes from the summary
def generate_notes(summary):
    notes = {
        "Title": "Audio Summary",
        "Paragraph": summary,
        "Conclusion": "This summary encapsulates the key points of the audio content.",
        "Bullet Points": [
            "Key idea 1",
            "Key idea 2",
            "Key idea 3"
        ]
    }
    return notes

# Function to create a PDF from structured notes
def create_pdf(notes, output_pdf):
    pdf = FPDF()
    pdf.add_page()

    # Title
    pdf.set_font("Arial", size=16)
    pdf.cell(200, 10, txt=notes["Title"], ln=True, align="C")

    # Paragraph
    pdf.set_font("Arial", size=12)
    pdf.multi_cell(0, 10, notes["Paragraph"])

    # Conclusion
    pdf.cell(200, 10, txt="Conclusion:", ln=True)
    pdf.multi_cell(0, 10, notes["Conclusion"])

    # Bullet Points
    pdf.cell(200, 10, txt="Bullet Points:", ln=True)
    for point in notes["Bullet Points"]:
        pdf.cell(200, 10, txt=f"- {point}", ln=True)

    pdf.output(output_pdf)

# Main workflow to integrate all steps
def main():
    mp3_file = upload_mp3_file()  # Upload MP3 file

    if not mp3_file:
        print("No file uploaded. Exiting...")
        return

    wav_file = convert_mp3_to_wav(mp3_file)  # Convert MP3 to WAV

    transcript = speech_to_text(wav_file)  # Convert speech to text

    if transcript:
        summary = summarize_text(transcript)  # Summarize transcript

        notes = generate_notes(summary)  # Generate structured notes

        output_pdf = "summary.pdf"

        create_pdf(notes, output_pdf)  # Create PDF

        print(f"PDF created: {output_pdf}")

        files.download(output_pdf)  # Provide download link for PDF
    else:
        print("Failed to generate transcript. Please check the audio file or model.")

# Run the project in Google Colab environment
if __name__ == "__main__":
    !pip install pydub vosk transformers fpdf

    main()